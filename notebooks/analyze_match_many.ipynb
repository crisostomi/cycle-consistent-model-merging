{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match and analyze many models\n",
    "---\n",
    "We load a set of pretrained models and match them cycle-consistently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/donato/Extra-storage/Code/model-merging/cycle-consistent-model-merging/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/media/donato/Extra-storage/Code/model-merging/cycle-consistent-model-merging/.venv/lib/python3.9/site-packages/pytorch_lightning/utilities/imports.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import itertools\n",
    "import logging\n",
    "import math\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "import json\n",
    "\n",
    "import hydra\n",
    "import matplotlib\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import omegaconf\n",
    "import pytorch_lightning\n",
    "import seaborn as sns\n",
    "import torch  # noqa\n",
    "import wandb\n",
    "from hydra.utils import instantiate\n",
    "from matplotlib import tri\n",
    "from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
    "from omegaconf import DictConfig\n",
    "from pytorch_lightning import LightningModule\n",
    "from scipy.stats import qmc\n",
    "from torch.utils.data import DataLoader, Subset, SubsetRandomSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nn_core.callbacks import NNTemplateCore\n",
    "from nn_core.common import PROJECT_ROOT\n",
    "from nn_core.common.utils import seed_index_everything\n",
    "from nn_core.model_logging import NNLogger\n",
    "\n",
    "import ccmm  # noqa\n",
    "from ccmm.matching.utils import (\n",
    "    apply_permutation_to_statedict,\n",
    "    get_all_symbols_combinations,\n",
    "    load_permutations,\n",
    "    perm_indices_to_perm_matrix,\n",
    "    plot_permutation_history_animation,\n",
    "    restore_original_weights,\n",
    ")\n",
    "from ccmm.utils.utils import (\n",
    "    fuse_batch_norm_into_conv,\n",
    "    get_interpolated_loss_acc_curves,\n",
    "    l2_norm_models,\n",
    "    linear_interpolate,\n",
    "    load_model_from_info,\n",
    "    map_model_seed_to_symbol,\n",
    "    normalize_unit_norm,\n",
    "    project_onto,\n",
    "    save_factored_permutations,\n",
    "    vector_to_state_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"text.usetex\": True,\n",
    "        \"font.family\": \"serif\",\n",
    "    }\n",
    ")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "cmap_name = \"coolwarm_r\"\n",
    "\n",
    "from ccmm.utils.plot import Palette\n",
    "\n",
    "palette = Palette(f\"{PROJECT_ROOT}/misc/palette2.json\")\n",
    "palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger(\"lightning.pytorch\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"torch\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"pytorch_lightning.accelerators.cuda\").setLevel(logging.WARNING)\n",
    "pylogger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import hydra\n",
    "from hydra import initialize, compose\n",
    "from typing import Dict, List\n",
    "\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "initialize(version_base=None, config_path=str(\"../conf\"), job_name=\"matching_n_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = compose(config_name=\"matching_n_models\", overrides=[\"model=resnet20\", \"model.widen_factor=2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_cfg = cfg  # NOQA\n",
    "cfg = cfg.matching\n",
    "\n",
    "seed_index_everything(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_samples = 5000\n",
    "num_train_samples = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = instantiate(core_cfg.dataset.test.transform)\n",
    "\n",
    "train_dataset = instantiate(core_cfg.dataset.train, transform=transform)\n",
    "test_dataset = instantiate(core_cfg.dataset.test, transform=transform)\n",
    "\n",
    "train_subset = Subset(train_dataset, list(range(num_train_samples)))\n",
    "train_loader = DataLoader(train_subset, batch_size=5000, num_workers=cfg.num_workers)\n",
    "\n",
    "test_subset = Subset(test_dataset, list(range(num_test_samples)))\n",
    "\n",
    "test_loader = DataLoader(test_subset, batch_size=1000, num_workers=cfg.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = instantiate(cfg.trainer, enable_progress_bar=False, enable_model_summary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ccmm.utils.utils import load_model_from_artifact\n",
    "\n",
    "run = wandb.init(project=core_cfg.core.project_name, entity=core_cfg.core.entity, job_type=\"matching\")\n",
    "\n",
    "# {a: 1, b: 2, c: 3, ..}\n",
    "symbols_to_seed: Dict[int, str] = {map_model_seed_to_symbol(seed): seed for seed in cfg.model_seeds}\n",
    "\n",
    "# TODO: remove ln from artifact path\n",
    "artifact_path = (\n",
    "    lambda seed: f\"{core_cfg.core.entity}/{core_cfg.core.project_name}/{core_cfg.dataset.name}_{core_cfg.model.model_identifier}_ln_{seed}:v0\"\n",
    ")\n",
    "\n",
    "# {a: model_a, b: model_b, c: model_c, ..}\n",
    "models: Dict[str, LightningModule] = {\n",
    "    map_model_seed_to_symbol(seed): load_model_from_artifact(run, artifact_path(seed)) for seed in cfg.model_seeds\n",
    "}\n",
    "\n",
    "num_models = len(models)\n",
    "\n",
    "pylogger.info(f\"Using {num_models} models with architecture {core_cfg.model.model_identifier}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# always permute the model having larger character order, i.e. c -> b, b -> a and so on ...\n",
    "symbols = set(symbols_to_seed.keys())\n",
    "sorted_symbols = sorted(symbols, reverse=False)\n",
    "\n",
    "# (a, b), (a, c), (b, c), ...\n",
    "all_combinations = get_all_symbols_combinations(symbols)\n",
    "# combinations of the form (a, b), (a, c), (b, c), .. and not (b, a), (c, a) etc\n",
    "canonical_combinations = [(source, target) for (source, target) in all_combinations if source < target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cycle-Consistent Matching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pylogger.info(f\"Matching the following model pairs: {canonical_combinations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load permutation specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_spec_builder = instantiate(core_cfg.model.permutation_spec_builder)\n",
    "permutation_spec = permutation_spec_builder.create_permutation_spec()\n",
    "\n",
    "ref_model = list(models.values())[0]\n",
    "assert set(permutation_spec.layer_and_axes_to_perm.keys()) == set(ref_model.model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = instantiate(cfg.matcher, permutation_spec=permutation_spec)\n",
    "pylogger.info(f\"Matcher: {matcher.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutations, perm_history = matcher(models, symbols=sorted_symbols, combinations=canonical_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {symb: model.to(\"cpu\") for symb, model in models.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permute models to universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ccmm.matching.utils import perm_matrix_to_perm_indices\n",
    "\n",
    "models_permuted_to_universe = {symbol: copy.deepcopy(model) for symbol, model in models.items()}\n",
    "\n",
    "for symbol, model in models_permuted_to_universe.items():\n",
    "    perms_to_universe = {}\n",
    "\n",
    "    for perm_name, perm in permutations[symbol].items():\n",
    "        perm = perm_indices_to_perm_matrix(perm)\n",
    "        perm_to_universe = perm.T\n",
    "        perm_to_universe = perm_matrix_to_perm_indices(perm_to_universe)\n",
    "        perms_to_universe[perm_name] = perm_to_universe\n",
    "\n",
    "    permuted_params = apply_permutation_to_statedict(permutation_spec, perms_to_universe, model.model.state_dict())\n",
    "    models_permuted_to_universe[symbol].model.load_state_dict(permuted_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permute models pairwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ccmm.matching.utils import unfactor_permutations\n",
    "\n",
    "models_permuted_pairwise = {\n",
    "    symbol: {other_symb: None for other_symb in set(symbols).difference(symbol)} for symbol in symbols\n",
    "}\n",
    "pairwise_permutations = unfactor_permutations(permutations)\n",
    "\n",
    "for fixed, permutee in all_combinations:\n",
    "    ref_model = copy.deepcopy(models[\"a\"])\n",
    "\n",
    "    permuted_params = apply_permutation_to_statedict(\n",
    "        permutation_spec, pairwise_permutations[fixed][permutee], models[permutee].model.state_dict()\n",
    "    )\n",
    "\n",
    "    ref_model.model.load_state_dict(permuted_params)\n",
    "    models_permuted_pairwise[fixed][permutee] = ref_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check performance of models before and after permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_perms = []\n",
    "after_perms = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for symbol, model in models_permuted_to_universe.items():\n",
    "    after_perm = trainer.test(models_permuted_to_universe[symbol], test_loader)[0][\"acc/test\"]\n",
    "    before_perm = trainer.test(models[symbol], test_loader)[0][\"acc/test\"]\n",
    "\n",
    "    before_perms.append(before_perm)\n",
    "    after_perms.append(after_perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(after_perms)\n",
    "print(before_perms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that permutation pairwise doesn't change performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(models[\"a\"], test_loader)[0][\"acc/test\"]\n",
    "trainer.test(models_permuted_pairwise[\"b\"][\"a\"], test_loader)[0][\"acc/test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Git Re-Basin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ccmm.matching.weight_matching import PermutationSpec, weight_matching\n",
    "from ccmm.matching.utils import get_inverse_permutations\n",
    "\n",
    "\n",
    "pairwise_perms_gitrebasin = {\n",
    "    symb: {other_symb: None for other_symb in set(symbols).difference(symb)} for symb in symbols\n",
    "}\n",
    "\n",
    "for fixed, permutee in canonical_combinations:\n",
    "    permutation = weight_matching(\n",
    "        permutation_spec,\n",
    "        fixed=models[fixed].model.state_dict(),\n",
    "        permutee=models[permutee].model.state_dict(),\n",
    "    )\n",
    "    pairwise_perms_gitrebasin[fixed][permutee] = permutation\n",
    "    pairwise_perms_gitrebasin[permutee][fixed] = get_inverse_permutations(permutation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze models as vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_symbs = {symbol: set(symbols).difference(symbol) for symbol in symbols}\n",
    "print(other_symbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_models = {symbol: torch.nn.utils.parameters_to_vector(model.parameters()) for symbol, model in models.items()}\n",
    "flat_models_permuted_to_universe = {\n",
    "    symbol: torch.nn.utils.parameters_to_vector(model.parameters())\n",
    "    for symbol, model in models_permuted_to_universe.items()\n",
    "}\n",
    "\n",
    "flat_models_permuted_pairwise = {\n",
    "    symbol: {\n",
    "        other_symb: torch.nn.utils.parameters_to_vector(models_permuted_pairwise[symbol][other_symb].parameters())\n",
    "        for other_symb in other_symbs[symbol]\n",
    "    }\n",
    "    for symbol in symbols\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for symbol in symbols:\n",
    "    flat_models_permuted_pairwise[symbol][symbol] = flat_models[symbol]\n",
    "    models_permuted_pairwise[symbol][symbol] = models[symbol]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that the permutations are cycle consistent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_names = list(pairwise_permutations[\"a\"][\"b\"].keys())\n",
    "\n",
    "for perm_name in perm_names:\n",
    "    P1 = perm_indices_to_perm_matrix(pairwise_permutations[\"a\"][\"b\"][perm_name])\n",
    "    P2 = perm_indices_to_perm_matrix(pairwise_permutations[\"b\"][\"c\"][perm_name])\n",
    "    P3 = perm_indices_to_perm_matrix(pairwise_permutations[\"c\"][\"a\"][perm_name])\n",
    "\n",
    "    cyclic_composition = P1 @ P2 @ P3\n",
    "    assert torch.allclose(cyclic_composition, torch.eye(P1.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots and tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ours = r\"$C^2M^3$\"\n",
    "label_gitrebasin = \"Git Re-Basin\"\n",
    "\n",
    "lambdas = np.linspace(0, 1, 25)\n",
    "\n",
    "get_curves = partial(\n",
    "    get_interpolated_loss_acc_curves, lambdas=lambdas, ref_model=ref_model, trainer=trainer, loader=test_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lmc(values, lambdas, labels, colors, axis=None):\n",
    "\n",
    "    num_curves = len(values)\n",
    "    transparencies = np.linspace(0.5, 1, num_curves)\n",
    "    linewidths = np.linspace(2.0, 4.0, num_curves)\n",
    "\n",
    "    for i, (val, label) in enumerate(zip(values, labels)):\n",
    "        if axis is None:\n",
    "            axis = plt\n",
    "\n",
    "        axis.plot(lambdas, val, label=label, alpha=transparencies[i], linewidth=linewidths[i], color=colors[i])\n",
    "\n",
    "\n",
    "plot_lmc = partial(plot_lmc, lambdas=lambdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_and_acc_curves(losses, accuracies, labels, output_name):\n",
    "\n",
    "    colors = palette.get_colors(len(labels))\n",
    "    fig, axes = plt.subplots(\n",
    "        1,\n",
    "        2,\n",
    "        figsize=(10, 3),\n",
    "    )\n",
    "    plot_lmc(accuracies, axis=axes[0], labels=labels, colors=colors)\n",
    "\n",
    "    axes[0].set_title(\"Accuracy\")\n",
    "    axes[0].set_xlabel(r\"$\\lambda$\")\n",
    "    axes[0].grid(True, alpha=0.3, linestyle=\"--\")\n",
    "\n",
    "    plot_lmc(losses, axis=axes[1], labels=labels, colors=colors)\n",
    "    axes[1].set_title(\"Loss\")\n",
    "    axes[1].set_xlabel(r\"$\\lambda$\")\n",
    "    axes[1].grid(True, alpha=0.3, linestyle=\"--\")\n",
    "\n",
    "    plt.subplots_adjust(bottom=0.1, right=0.8, top=0.9, wspace=0.4)\n",
    "\n",
    "    legend_y = -0.5 if len(labels) > 3 else -0.4\n",
    "    legend_x = -0.2\n",
    "    plt.legend(bbox_to_anchor=(legend_x, legend_y), loc=\"center\", ncol=3)\n",
    "    plt.savefig(f\"figures/{output_name}.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLOT: LMC of a $A$ and $A \\rightarrow B \\rightarrow C \\rightarrow A$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclic_permute(pairwise_perms, symbols, models):\n",
    "    \"\"\"\n",
    "    Applies a cycle of permutations to the first model in models and returns the resulting model.\n",
    "    \"\"\"\n",
    "    ordered_symbs = sorted(list(symbols))\n",
    "    model_current = models[ordered_symbs[0]].model.state_dict()\n",
    "\n",
    "    for i, symb in enumerate(ordered_symbs[1:] + [ordered_symbs[0]]):\n",
    "        print(\"next: {} -- prev: {}\".format(symb, ordered_symbs[i]))\n",
    "        permutation = pairwise_perms[symb][ordered_symbs[i]]\n",
    "        model_current = apply_permutation_to_statedict(permutation_spec, permutation, model_current)\n",
    "\n",
    "    return model_current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_cycle_ours = cyclic_permute(pairwise_permutations, [\"a\", \"b\", \"c\"], models)\n",
    "a_cycle_gitr = cyclic_permute(pairwise_perms_gitrebasin, [\"a\", \"b\", \"c\"], models)\n",
    "\n",
    "initial_model = models[\"a\"]\n",
    "permuted_model_ours = copy.deepcopy(initial_model)\n",
    "permuted_model_ours.model.load_state_dict(a_cycle_ours)\n",
    "permuted_model_gitr = copy.deepcopy(initial_model)\n",
    "permuted_model_gitr.model.load_state_dict(a_cycle_gitr)\n",
    "\n",
    "loss_cycle_ours, acc_cycle_ours = get_curves(\n",
    "    model_a=initial_model,\n",
    "    model_b=permuted_model_ours,\n",
    ")\n",
    "loss_cycle_gitr, acc_cycle_gitr = get_curves(\n",
    "    model_a=initial_model,\n",
    "    model_b=permuted_model_gitr,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [label_gitrebasin, label_ours]\n",
    "losses = [loss_cycle_gitr, loss_cycle_ours]\n",
    "accuracies = [acc_cycle_gitr, acc_cycle_ours]\n",
    "\n",
    "plot_loss_and_acc_curves(losses, accuracies, labels, \"lmc_a_cycled_a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLOT: LMC in the original space and in the universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ac, acc_ac = get_curves(\n",
    "    model_a=models[\"a\"],\n",
    "    model_b=models[\"c\"],\n",
    ")\n",
    "\n",
    "loss_a_univ_c_univ, acc_a_univ_c_univ = get_curves(\n",
    "    model_a=models_permuted_to_universe[\"a\"],\n",
    "    model_b=models_permuted_to_universe[\"c\"],\n",
    ")\n",
    "\n",
    "loss_bc, acc_bc = get_curves(\n",
    "    model_a=models[\"b\"],\n",
    "    model_b=models[\"c\"],\n",
    ")\n",
    "\n",
    "loss_b_univ_c_univ, acc_b_univ_c_univ = get_curves(\n",
    "    model_a=models_permuted_to_universe[\"b\"],\n",
    "    model_b=models_permuted_to_universe[\"c\"],\n",
    ")\n",
    "\n",
    "loss_ab, acc_ab = get_curves(\n",
    "    model_a=models[\"a\"],\n",
    "    model_b=models[\"b\"],\n",
    ")\n",
    "\n",
    "loss_a_univ_b_univ, acc_a_univ_b_univ = get_curves(\n",
    "    model_a=models_permuted_to_universe[\"a\"],\n",
    "    model_b=models_permuted_to_universe[\"b\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_endpoints = [r\"$A$\", r\"$P_A^{\\top} A$\", \"$A$\", r\"$P_A^{\\top} A$\", \"$B$\", r\"$P_B{^\\top} B$\"]\n",
    "right_endpoints = [\"C\", r\"$P_C^{\\top} C$\", \"$B$\", r\"$P_B^{\\top} B$\", \"$C$\", r\"$P_C{^\\top} C$\"]\n",
    "\n",
    "labels = [f\"{left} - {right}\" for left, right in zip(left_endpoints, right_endpoints)]\n",
    "\n",
    "losses = [loss_ac, loss_a_univ_c_univ, loss_ab, loss_a_univ_b_univ, loss_bc, loss_b_univ_c_univ]\n",
    "accuracies = [acc_ac, acc_a_univ_c_univ, acc_ab, acc_a_univ_b_univ, acc_bc, acc_b_univ_c_univ]\n",
    "\n",
    "plot_loss_and_acc_curves(losses, accuracies, labels, \"interp_curves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {\n",
    "    \"ac\": loss_ac,\n",
    "    \"a_univ_c_univ\": loss_a_univ_c_univ,\n",
    "    \"ab\": loss_ab,\n",
    "    \"a_univ_b_univ\": loss_a_univ_b_univ,\n",
    "    \"bc\": loss_bc,\n",
    "    \"b_univ_c_univ\": loss_b_univ_c_univ,\n",
    "}\n",
    "\n",
    "json.dump(losses, open(\"results/losses.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TABLE: Accumulated error in cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.linspace(0, 1, 3)\n",
    "\n",
    "get_curves = partial(\n",
    "    get_interpolated_loss_acc_curves, lambdas=lambdas, ref_model=ref_model, trainer=trainer, loader=test_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ccmm.utils.utils import cosine_models\n",
    "\n",
    "\n",
    "def get_accumulated_error(pairwise_perms, models, cycle_len=3, distance=\"l2\"):\n",
    "\n",
    "    symbols = list(pairwise_perms.keys())\n",
    "\n",
    "    cycles = list(itertools.permutations(symbols))\n",
    "\n",
    "    output = {}\n",
    "\n",
    "    for c in cycles:\n",
    "        print(f\"Cycle: {c}\")\n",
    "\n",
    "        key = \"\".join(c)\n",
    "        model_c = cyclic_permute(pairwise_perms, list(c), models)\n",
    "        ordered_cycle = sorted(list(c))\n",
    "\n",
    "        initial_model = models[ordered_cycle[0]]\n",
    "\n",
    "        permuted_model = copy.deepcopy(initial_model)\n",
    "        permuted_model.model.load_state_dict(model_c)\n",
    "\n",
    "        if distance == \"l2\":\n",
    "            print(f\"Model distance: {l2_norm_models(model_c, initial_model.model.state_dict())}\")\n",
    "\n",
    "        elif distance == \"cosine\":\n",
    "            print(f\"Model similarity: {cosine_models(model_c, initial_model.model.state_dict())}\")\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown distance metric: {distance}\")\n",
    "\n",
    "        losses, accs = get_curves(model_a=initial_model, model_b=permuted_model)\n",
    "\n",
    "        output[key] = {\"x\": lambdas, \"loss\": np.array(losses), \"acc\": np.array(accs)}\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_accumulated_error(pairwise_perms_gitrebasin, models, cycle_len=3, distance=\"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_accumulated_error(pairwise_permutations, models, cycle_len=3, distance=\"cosine\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccmm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
